# Les **cas pratiques Big Data**

 Les **cas pratiques Big Data**, class√©s par niveau (1 √† 6), avec **user stories**, **objectifs p√©dagogiques**, **livrables**, et **technos/outils utilis√©s**. Ce backlog peut servir de base pour un projet de formation, un bootcamp, ou une autoformation intensive.



### üì¶ Contenu d√©taill√© :

| Niveau      | Dossier                      | Fichiers inclus                                                   |
| ----------- | ---------------------------- | ----------------------------------------------------------------- |
| Niveau 1    | `niveau_1_fondamentaux/`     | `linux_basics.sh`, `python_intro.ipynb`, `java_hello_world.java`  |
| Niveau 1bis | `niveau_1bis_bi_end_to_end/` | `etl_process.sql`, `powerbi_dashboard.pbix`, `airflow_dag_etl.py` |
| Niveau 2    | `niveau_2_architecture/`     | `mapreduce_wordcount.py`, `hdfs_io.py`                            |
| Niveau 3    | `niveau_3_streaming/`        | `kafka_producer.py`, `spark_streaming_twitter.py`                 |
| Niveau 4    | `niveau_4_visualisation/`    | `hive_query.sql`, `superset_dashboard_setup.md`                   |
| Niveau 5    | `niveau_5_ml/`               | `spark_mllib_churn_prediction.py`, `mlflow_tracking_demo.py`      |
| Niveau 6    | `niveau_6_devops/`           | `docker_compose_spark.yml`, `grafana_dashboard.json`              |



---

## üîπ **Niveau 0 : Fondamentaux**

### üéØ Objectif : Acqu√©rir les bases en Linux, Python et Java pour manipuler des outils Big Data.

| ID   | User Story                                                                                                    | Description                                                                     | Livrables               | Outils                  |
| ---- | ------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------- | ----------------------- | ----------------------- |
| N1-1 | En tant qu'apprenant, je veux manipuler les commandes Linux de base pour pr√©parer mon environnement Big Data. | Script d'automatisation de setup, manipulation de fichiers, users, droits, SSH. | `linux_basics.sh`       | Bash, Linux             |
| N1-2 | En tant qu‚Äôapprenant, je veux √©crire un script Python simple pour lire et analyser un fichier CSV.            | Notions de variables, boucles, fonctions, pandas.                               | `python_intro.ipynb`    | Python, Jupyter, pandas |
| N1-3 | En tant qu‚Äôapprenant, je veux compiler et ex√©cuter mon premier programme Java.                                | Hello World, gestion de classes, compilation manuelle.                          | `java_hello_world.java` | Java JDK                |


-----------
Voici une **extension compl√®te du Niveau 1 : Fondamentaux des Donn√©es** √† int√©grer dans la **feuille de route Big Data** sous forme de **backlog d√©taill√©**, avec **comp√©tences √† acqu√©rir**, **ressources**, **cas pratiques** et **user stories**. Ce niveau est con√ßu pour durer **3 √† 4 semaines**, id√©al pour b√¢tir une base solide avant d‚Äôaborder l‚Äôarchitecture Big Data.

---

## üîπ **Niveau 1 ‚Äì Fondamentaux des donn√©es (3‚Äì4 semaines)**

üéØ **Objectif global** : Comprendre les structures de donn√©es, les formats, les bases relationnelles et NoSQL, ainsi que les mod√®les de donn√©es.

---

### üóÑÔ∏è **1. Bases de donn√©es relationnelles (SQL)**

#### ‚úÖ Comp√©tences √† acqu√©rir :

* Ma√Ætriser les requ√™tes SQL de base : `SELECT`, `INSERT`, `UPDATE`, `DELETE`
* Effectuer des **jointures**, des **agr√©gations**, des **sous-requ√™tes**
* Appliquer les **principes de normalisation**

#### üìö Ressources recommand√©es :

* [SQLZoo](https://sqlzoo.net/)
* [W3Schools SQL Tutorial](https://www.w3schools.com/sql/)

#### üß™ Cas pratique :

**Cr√©er une base de donn√©es pour une boutique en ligne**

* Tables : Clients, Produits, Commandes
* Ajouter des donn√©es fictives
* Requ√™tes : ventes mensuelles, top produits, clients inactifs, panier moyen

#### üßæ User Stories :

| ID       | User Story                                                                                                      | Livrables                           | Outils              |
| -------- | --------------------------------------------------------------------------------------------------------------- | ----------------------------------- | ------------------- |
| N1-SQL-1 | En tant qu'utilisateur, je veux interroger les ventes mensuelles pour suivre l‚Äô√©volution du chiffre d‚Äôaffaires. | `boutique.sql`, rapport de requ√™tes | SQLite / PostgreSQL |
| N1-SQL-2 | En tant qu‚Äôanalyste, je veux trouver les produits les plus vendus par cat√©gorie.                                | `requetes_top_ventes.sql`           | SQL                 |
| N1-SQL-3 | En tant qu‚Äôadmin, je veux structurer ma base selon la 3NF.                                                      | `schema_normalise.pdf`              | dbdiagram.io        |

---

### üóÉÔ∏è **2. Bases de donn√©es NoSQL**

#### ‚úÖ Comp√©tences √† acqu√©rir :

* Diff√©rences entre relationnel et NoSQL
* Manipulation de documents **JSON** avec **MongoDB**
* Introduction au stockage en m√©moire avec **Redis**

#### üìö Ressources recommand√©es :

* [MongoDB University](https://university.mongodb.com/)
* [Redis Tutorials](https://redis.io/docs/)

#### üß™ Cas pratique :

**Impl√©menter un catalogue de produits NoSQL et un syst√®me de cache**

* Base MongoDB : Produits (nom, prix, stock, cat√©gories)
* Ajout de recherche filtr√©e
* Mise en cache des produits populaires avec Redis

#### üßæ User Stories :

| ID         | User Story                                                                                      | Livrables                  | Outils        |
| ---------- | ----------------------------------------------------------------------------------------------- | -------------------------- | ------------- |
| N1-NOSQL-1 | En tant que d√©veloppeur, je veux stocker mes produits dans une collection MongoDB.              | `mongo_insert_products.js` | MongoDB       |
| N1-NOSQL-2 | En tant qu‚Äôanalyste, je veux rechercher les produits disponibles dans une cat√©gorie sp√©cifique. | `mongo_query_products.js`  | MongoDB       |
| N1-NOSQL-3 | En tant que devOps, je veux mettre en cache les produits populaires avec Redis.                 | `redis_cache.py`           | Redis, Python |

---

### üìÑ **3. Formats de donn√©es (CSV, JSON, Parquet, Avro)**

#### ‚úÖ Comp√©tences √† acqu√©rir :

* Comprendre les diff√©rences : **CSV (simple)**, **JSON (hi√©rarchique)**, **Avro/Parquet (optimis√©s Big Data)**
* Convertir les formats en Python

#### üìö Ressources recommand√©es :

* [DataFlair ‚Äì Big Data File Formats](https://data-flair.training/blogs/file-formats-in-hadoop/)

#### üß™ Cas pratique :

**Convertir un fichier CSV des ventes en format Parquet et Avro**

* Lecture/√©criture avec pandas, pyarrow, fastavro
* Comparaison de la taille et de la vitesse de lecture

#### üßæ User Stories :

| ID          | User Story                                                                                                | Livrables              | Outils               |
| ----------- | --------------------------------------------------------------------------------------------------------- | ---------------------- | -------------------- |
| N1-FORMAT-1 | En tant que data engineer, je veux convertir mes fichiers CSV en Parquet pour l‚Äôoptimisation du stockage. | `csv_to_parquet.py`    | Python, pyarrow      |
| N1-FORMAT-2 | En tant que d√©veloppeur, je veux comparer la taille de fichiers JSON et Avro.                             | `format_comparator.py` | fastavro, JSON, Avro |

---

### üß† **4. Mod√©lisation de donn√©es**

#### ‚úÖ Comp√©tences √† acqu√©rir :

* Cr√©er des **mod√®les conceptuels**, **logiques** et **physiques**
* Utiliser des outils de mod√©lisation : **dbdiagram.io**, **Lucidchart**
* Notions d‚Äô**entrep√¥t de donn√©es** et sch√©ma en **√©toile** et **flocon**

#### üìö Ressources recommand√©es :

* [dbdiagram.io](https://dbdiagram.io/)
* [Lucidchart](https://www.lucidchart.com/pages/)

#### üß™ Cas pratique :

**Mod√©liser un entrep√¥t de donn√©es pour une entreprise e-commerce**

* Faits : Commandes
* Dimensions : Clients, Produits, Dates
* Mod√®le √©toile vs flocon

#### üßæ User Stories :

| ID         | User Story                                                                                            | Livrables               | Outils       |
| ---------- | ----------------------------------------------------------------------------------------------------- | ----------------------- | ------------ |
| N1-MODEL-1 | En tant qu‚Äôanalyste, je veux mod√©liser un entrep√¥t de donn√©es e-commerce avec des dimensions claires. | `modele_entrepot.png`   | dbdiagram.io |
| N1-MODEL-2 | En tant que concepteur, je veux comparer les mod√®les en √©toile et en flocon.                          | `comparatif_modeles.md` | Lucidchart   |

---

## üìÅ Contenu √† ajouter dans le pack `.zip` :

| Dossier                  | Fichiers sugg√©r√©s                                                 |
| ------------------------ | ----------------------------------------------------------------- |
| `niveau_1_sql/`          | `boutique.sql`, `requetes_top_ventes.sql`, `schema_normalise.pdf` |
| `niveau_1_nosql/`        | `mongo_insert_products.js`, `redis_cache.py`                      |
| `niveau_1_formats/`      | `csv_to_parquet.py`, `format_comparator.py`                       |
| `niveau_1_modelisation/` | `modele_entrepot.png`, `comparatif_modeles.md`                    |



---

## üîπ **Niveau 1bis : BI end-to-end**

### üéØ Objectif : Cr√©er une cha√Æne de Business Intelligence compl√®te (ETL + visualisation).

| ID    | User Story                                                                                        | Description                                  | Livrables                | Outils                  |
| ----- | ------------------------------------------------------------------------------------------------- | -------------------------------------------- | ------------------------ | ----------------------- |
| N1B-1 | En tant qu‚Äôanalyste, je veux construire un script SQL d‚ÄôETL pour transformer des donn√©es clients. | Nettoyage, transformation, agr√©gation.       | `etl_process.sql`        | SQL (PostgreSQL, MySQL) |
| N1B-2 | En tant qu‚Äôanalyste, je veux cr√©er un dashboard Power BI √† partir des donn√©es nettoy√©es.          | KPI, graphiques dynamiques, filtres.         | `powerbi_dashboard.pbix` | Power BI                |
| N1B-3 | En tant qu‚Äôadmin data, je veux automatiser l‚ÄôETL avec Airflow.                                    | Cr√©ation d‚Äôun DAG d‚Äôorchestration quotidien. | `airflow_dag_etl.py`     | Apache Airflow          |

---

## üîπ **Niveau 2 : Architecture Big Data**

### üéØ Objectif : Comprendre les composants et pipelines de traitement Big Data.

| ID   | User Story                                                                                   | Description                         | Livrables                | Outils         |
| ---- | -------------------------------------------------------------------------------------------- | ----------------------------------- | ------------------------ | -------------- |
| N2-1 | En tant qu‚Äôapprenant, je veux d√©velopper un MapReduce pour compter les mots dans un fichier. | Logique de map, reduce, test local. | `mapreduce_wordcount.py` | Python, Hadoop |
| N2-2 | En tant que d√©veloppeur, je veux lire et √©crire des fichiers dans HDFS.                      | Op√©rations HDFS API et CLI.         | `hdfs_io.py`             | Hadoop HDFS    |

---

## üîπ **Niveau 3 : Streaming & Temps R√©el**

### üéØ Objectif : Collecter, traiter et analyser des donn√©es en temps r√©el.

| ID   | User Story                                                                                     | Description                            | Livrables                    | Outils                    |
| ---- | ---------------------------------------------------------------------------------------------- | -------------------------------------- | ---------------------------- | ------------------------- |
| N3-1 | En tant que d√©veloppeur, je veux produire des messages Kafka √† partir d‚Äôune source de donn√©es. | Simulation de logs ou √©v√©nements JSON. | `kafka_producer.py`          | Apache Kafka              |
| N3-2 | En tant que data engineer, je veux consommer ces donn√©es avec Spark Streaming.                 | Traitement temps r√©el avec analyse.    | `spark_streaming_twitter.py` | Apache Spark, Twitter API |

---

## üîπ **Niveau 4 : Stockage, Requ√™tage, Visualisation**

### üéØ Objectif : Stocker, interroger et visualiser efficacement les donn√©es.

| ID   | User Story                                                                   | Description                           | Livrables                     | Outils          |
| ---- | ---------------------------------------------------------------------------- | ------------------------------------- | ----------------------------- | --------------- |
| N4-1 | En tant que data analyst, je veux √©crire des requ√™tes Hive sur un data lake. | Table EXTERNAL, GROUP BY, JOINs.      | `hive_query.sql`              | Apache Hive     |
| N4-2 | En tant qu‚Äôutilisateur m√©tier, je veux visualiser les KPIs via Superset.     | Cr√©ation de dashboards, KPI, graphes. | `superset_dashboard_setup.md` | Apache Superset |

---

## üîπ **Niveau 5 : Machine Learning sur Big Data**

### üéØ Objectif : Appliquer du Machine Learning distribu√© avec Spark et suivre les mod√®les.

| ID   | User Story                                                                        | Description                                    | Livrables                         | Outils        |
| ---- | --------------------------------------------------------------------------------- | ---------------------------------------------- | --------------------------------- | ------------- |
| N5-1 | En tant que data scientist, je veux pr√©dire les d√©parts clients avec Spark MLlib. | Pipeline ML avec features, mod√®le, √©valuation. | `spark_mllib_churn_prediction.py` | Spark MLlib   |
| N5-2 | En tant que MLOps, je veux tracer les exp√©riences ML avec MLFlow.                 | Tracking des mod√®les, metrics, versionning.    | `mlflow_tracking_demo.py`         | MLFlow, Spark |

---

## üîπ **Niveau 6 : DevOps & S√©curit√©**

### üéØ Objectif : D√©ployer, monitorer et s√©curiser une architecture Big Data.

| ID   | User Story                                                                      | Description                           | Livrables                  | Outils              |
| ---- | ------------------------------------------------------------------------------- | ------------------------------------- | -------------------------- | ------------------- |
| N6-1 | En tant qu‚Äôadmin, je veux d√©ployer un cluster Spark local avec Docker Compose.  | Services Spark, Master/Worker.        | `docker_compose_spark.yml` | Docker Compose      |
| N6-2 | En tant qu‚Äôops, je veux cr√©er un dashboard Grafana pour surveiller Kafka/Spark. | Int√©gration avec Prometheus, metrics. | `grafana_dashboard.json`   | Grafana, Prometheus |

---

## üìö Ressources pour accompagner chaque niveau :

* **Docs officielles** : Apache (Kafka, Spark, Hive, Airflow), Docker, Superset.
* **Cours** : Data Engineering on GCP (Coursera), Datacamp - Big Data Fundamentals.
* **Livres** :

  * *Designing Data-Intensive Applications* ‚Äì Martin Kleppmann
  * *Streaming Systems* ‚Äì Tyler Akidau

---

