# Les **cas pratiques Big Data**

 Les **cas pratiques Big Data**, classÃ©s par niveau (1 Ã  6), avec **user stories**, **objectifs pÃ©dagogiques**, **livrables**, et **technos/outils utilisÃ©s**. Ce backlog peut servir de base pour un projet de formation, un bootcamp, ou une autoformation intensive.



### ðŸ“¦ Contenu dÃ©taillÃ© :

| Niveau      | Dossier                      | Fichiers inclus                                                   |
| ----------- | ---------------------------- | ----------------------------------------------------------------- |
| Niveau 1    | `niveau_1_fondamentaux/`     | `linux_basics.sh`, `python_intro.ipynb`, `java_hello_world.java`  |
| Niveau 1bis | `niveau_1bis_bi_end_to_end/` | `etl_process.sql`, `powerbi_dashboard.pbix`, `airflow_dag_etl.py` |
| Niveau 2    | `niveau_2_architecture/`     | `mapreduce_wordcount.py`, `hdfs_io.py`                            |
| Niveau 3    | `niveau_3_streaming/`        | `kafka_producer.py`, `spark_streaming_twitter.py`                 |
| Niveau 4    | `niveau_4_visualisation/`    | `hive_query.sql`, `superset_dashboard_setup.md`                   |
| Niveau 5    | `niveau_5_ml/`               | `spark_mllib_churn_prediction.py`, `mlflow_tracking_demo.py`      |
| Niveau 6    | `niveau_6_devops/`           | `docker_compose_spark.yml`, `grafana_dashboard.json`              |



---

## ðŸ”¹ **Niveau 1 : Fondamentaux**

### ðŸŽ¯ Objectif : AcquÃ©rir les bases en Linux, Python et Java pour manipuler des outils Big Data.

| ID   | User Story                                                                                                    | Description                                                                     | Livrables               | Outils                  |
| ---- | ------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------- | ----------------------- | ----------------------- |
| N1-1 | En tant qu'apprenant, je veux manipuler les commandes Linux de base pour prÃ©parer mon environnement Big Data. | Script d'automatisation de setup, manipulation de fichiers, users, droits, SSH. | `linux_basics.sh`       | Bash, Linux             |
| N1-2 | En tant quâ€™apprenant, je veux Ã©crire un script Python simple pour lire et analyser un fichier CSV.            | Notions de variables, boucles, fonctions, pandas.                               | `python_intro.ipynb`    | Python, Jupyter, pandas |
| N1-3 | En tant quâ€™apprenant, je veux compiler et exÃ©cuter mon premier programme Java.                                | Hello World, gestion de classes, compilation manuelle.                          | `java_hello_world.java` | Java JDK                |

---

## ðŸ”¹ **Niveau 1bis : BI end-to-end**

### ðŸŽ¯ Objectif : CrÃ©er une chaÃ®ne de Business Intelligence complÃ¨te (ETL + visualisation).

| ID    | User Story                                                                                        | Description                                  | Livrables                | Outils                  |
| ----- | ------------------------------------------------------------------------------------------------- | -------------------------------------------- | ------------------------ | ----------------------- |
| N1B-1 | En tant quâ€™analyste, je veux construire un script SQL dâ€™ETL pour transformer des donnÃ©es clients. | Nettoyage, transformation, agrÃ©gation.       | `etl_process.sql`        | SQL (PostgreSQL, MySQL) |
| N1B-2 | En tant quâ€™analyste, je veux crÃ©er un dashboard Power BI Ã  partir des donnÃ©es nettoyÃ©es.          | KPI, graphiques dynamiques, filtres.         | `powerbi_dashboard.pbix` | Power BI                |
| N1B-3 | En tant quâ€™admin data, je veux automatiser lâ€™ETL avec Airflow.                                    | CrÃ©ation dâ€™un DAG dâ€™orchestration quotidien. | `airflow_dag_etl.py`     | Apache Airflow          |

---

## ðŸ”¹ **Niveau 2 : Architecture Big Data**

### ðŸŽ¯ Objectif : Comprendre les composants et pipelines de traitement Big Data.

| ID   | User Story                                                                                   | Description                         | Livrables                | Outils         |
| ---- | -------------------------------------------------------------------------------------------- | ----------------------------------- | ------------------------ | -------------- |
| N2-1 | En tant quâ€™apprenant, je veux dÃ©velopper un MapReduce pour compter les mots dans un fichier. | Logique de map, reduce, test local. | `mapreduce_wordcount.py` | Python, Hadoop |
| N2-2 | En tant que dÃ©veloppeur, je veux lire et Ã©crire des fichiers dans HDFS.                      | OpÃ©rations HDFS API et CLI.         | `hdfs_io.py`             | Hadoop HDFS    |

---

## ðŸ”¹ **Niveau 3 : Streaming & Temps RÃ©el**

### ðŸŽ¯ Objectif : Collecter, traiter et analyser des donnÃ©es en temps rÃ©el.

| ID   | User Story                                                                                     | Description                            | Livrables                    | Outils                    |
| ---- | ---------------------------------------------------------------------------------------------- | -------------------------------------- | ---------------------------- | ------------------------- |
| N3-1 | En tant que dÃ©veloppeur, je veux produire des messages Kafka Ã  partir dâ€™une source de donnÃ©es. | Simulation de logs ou Ã©vÃ©nements JSON. | `kafka_producer.py`          | Apache Kafka              |
| N3-2 | En tant que data engineer, je veux consommer ces donnÃ©es avec Spark Streaming.                 | Traitement temps rÃ©el avec analyse.    | `spark_streaming_twitter.py` | Apache Spark, Twitter API |

---

## ðŸ”¹ **Niveau 4 : Stockage, RequÃªtage, Visualisation**

### ðŸŽ¯ Objectif : Stocker, interroger et visualiser efficacement les donnÃ©es.

| ID   | User Story                                                                   | Description                           | Livrables                     | Outils          |
| ---- | ---------------------------------------------------------------------------- | ------------------------------------- | ----------------------------- | --------------- |
| N4-1 | En tant que data analyst, je veux Ã©crire des requÃªtes Hive sur un data lake. | Table EXTERNAL, GROUP BY, JOINs.      | `hive_query.sql`              | Apache Hive     |
| N4-2 | En tant quâ€™utilisateur mÃ©tier, je veux visualiser les KPIs via Superset.     | CrÃ©ation de dashboards, KPI, graphes. | `superset_dashboard_setup.md` | Apache Superset |

---

## ðŸ”¹ **Niveau 5 : Machine Learning sur Big Data**

### ðŸŽ¯ Objectif : Appliquer du Machine Learning distribuÃ© avec Spark et suivre les modÃ¨les.

| ID   | User Story                                                                        | Description                                    | Livrables                         | Outils        |
| ---- | --------------------------------------------------------------------------------- | ---------------------------------------------- | --------------------------------- | ------------- |
| N5-1 | En tant que data scientist, je veux prÃ©dire les dÃ©parts clients avec Spark MLlib. | Pipeline ML avec features, modÃ¨le, Ã©valuation. | `spark_mllib_churn_prediction.py` | Spark MLlib   |
| N5-2 | En tant que MLOps, je veux tracer les expÃ©riences ML avec MLFlow.                 | Tracking des modÃ¨les, metrics, versionning.    | `mlflow_tracking_demo.py`         | MLFlow, Spark |

---

## ðŸ”¹ **Niveau 6 : DevOps & SÃ©curitÃ©**

### ðŸŽ¯ Objectif : DÃ©ployer, monitorer et sÃ©curiser une architecture Big Data.

| ID   | User Story                                                                      | Description                           | Livrables                  | Outils              |
| ---- | ------------------------------------------------------------------------------- | ------------------------------------- | -------------------------- | ------------------- |
| N6-1 | En tant quâ€™admin, je veux dÃ©ployer un cluster Spark local avec Docker Compose.  | Services Spark, Master/Worker.        | `docker_compose_spark.yml` | Docker Compose      |
| N6-2 | En tant quâ€™ops, je veux crÃ©er un dashboard Grafana pour surveiller Kafka/Spark. | IntÃ©gration avec Prometheus, metrics. | `grafana_dashboard.json`   | Grafana, Prometheus |

---

## ðŸ“š Ressources pour accompagner chaque niveau :

* **Docs officielles** : Apache (Kafka, Spark, Hive, Airflow), Docker, Superset.
* **Cours** : Data Engineering on GCP (Coursera), Datacamp - Big Data Fundamentals.
* **Livres** :

  * *Designing Data-Intensive Applications* â€“ Martin Kleppmann
  * *Streaming Systems* â€“ Tyler Akidau

---

